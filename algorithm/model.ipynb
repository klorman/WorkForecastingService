{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import scipy\n",
    "from scipy import stats\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import transformers as ppb"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_1 = pd.read_excel(\"1_Многоквартирные дома с технико-экономическими характеристиками.xlsx\")\n",
    "table_2 = pd.read_excel(\"2_Инциденты,_зарегистрированные_на_объектах_городского_хозяйства.xlsx\")\n",
    "table_3 = pd.read_excel(\"3_Работы по капитальному ремонту, проведенные в многоквартирных домах.xlsx\")\n",
    "table_4_1 = pd.read_excel(\"4_Виды работ по капитальному ремонту многоквартирных домов.xlsx\")\n",
    "table_4_2 = pd.read_excel(\"4_Виды работ по содержанию и ремонту общего имущества многоквартирных домов.xlsx\")\n",
    "table_5 = pd.read_excel(\"5_Типы событий, регистрируемых по типу объекта многоквартирный дом.xlsx\")\n",
    "\n",
    "table_1 = table_1.drop(labels=[0], axis=0)\n",
    "table_4_1 = table_4_1.drop(labels=[0], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All date make date\n",
    "#table 2\n",
    "table_2[\"Дата создания во внешней системе\"] = pd.to_datetime(table_2[\"Дата создания во внешней системе\"], \n",
    "                                                             format = \"%Y-%m-%d %H:%M:%S.%f\",\n",
    "                                                             errors = 'coerce')\n",
    "table_2[\"Дата закрытия\"] = pd.to_datetime(table_2[\"Дата создания во внешней системе\"], \n",
    "                                          format = \"%Y-%m-%d %H:%M:%S.%f\",\n",
    "                                          errors = 'coerce')\n",
    "#table 3\n",
    "table_3[\"PLAN_DATE_START\"] = pd.to_datetime(table_3[\"PLAN_DATE_START\"],\n",
    "                                            #format=\"%Y.%m.%d\",\n",
    "                                            errors='coerce')\n",
    "table_3[\"PLAN_DATE_END\"] = pd.to_datetime(table_3[\"PLAN_DATE_END\"],\n",
    "                                            #format=\"%Y.%m.%d\",\n",
    "                                            errors='ignore')\n",
    "table_3[\"FACT_DATE_START\"] = pd.to_datetime(table_3[\"FACT_DATE_START\"],\n",
    "                                            #format=\"%Y.%m.%d\",\n",
    "                                            errors='ignore')\n",
    "table_3[\"FACT_DATE_END\"] = pd.to_datetime(table_3[\"FACT_DATE_END\"],\n",
    "                                            #format=\"%Y.%m.%d\",\n",
    "                                            errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function standartization\n",
    "def StandardCol(table: pd.DataFrame, name: str):\n",
    "    table[name] = table[name].astype(float)\n",
    "    if table_1[name].isna().sum() != 0:\n",
    "        table[name] = table[name].fillna(0)\n",
    "    return stats.zscore(table[name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BERT tokenizer\n",
    "model_class, tokenizer_class, pretrained_weights = (ppb.DistilBertModel, ppb.DistilBertTokenizer, 'distilbert-base-uncased')\n",
    "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
    "\n",
    "# удаление нахер вссего NaN дерьма\n",
    "for count, elem in enumerate(table_2[\"Наименование\"].isna()):\n",
    "    if elem:\n",
    "        table_2 = table_2.drop(labels=[count], axis=0)\n",
    "table_2[\"Наименование\"].isna().sum()\n",
    "\n",
    "# apply tokenizator\n",
    "df = table_2[\"Наименование\"].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))\n",
    "\n",
    "# find max_len\n",
    "max_len = 0\n",
    "for i in df.values:\n",
    "    if len(i) > max_len:\n",
    "        max_len = len(i)\n",
    "\n",
    "if max_len % 2 == 1:\n",
    "    max_len += 1\n",
    "\n",
    "# add <pad> symbol\n",
    "padded = np.array([i + [0]*(max_len-len(i)) for i in df.values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_batch = []\n",
    "y_batch = []\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, table_1, table_2, table_3, padded = None):\n",
    "        super().__init__()\n",
    "        # make Bert Model for make embedding requests\n",
    "        model_class, tokenizer_class, pretrained_weights = (ppb.DistilBertModel, ppb.DistilBertTokenizer, 'distilbert-base-uncased')\n",
    "        # Load tokenizer\n",
    "        self.tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
    "        self.features_table_1 = [ \"COL_756\", \"COL_758\", \"COL_759\", \"COL_760\", \"COL_761\", \"COL_762\", \n",
    "                              \"COL_763\", \"COL_764\", \"COL_769\", \"COL_770\", \"COL_771\", \"COL_772\", \n",
    "                              \"COL_781\", \"COL_3363\"]\n",
    "        self.features_table_2 = [ \"Дата закрытия\", \"Дата создания во внешней системе\"]\n",
    "        self.features_table_3 = [\"WORK_NAME\", \"PLAN_DATE_START\", \"FACT_DATE_START\"] # можно ещё \"PLAN_DATE_START\", \"FACT_DATE_START\"\n",
    "        self.unique_unom = list(set(table_1[\"COL_782\"].apply(int)) & set(table_2[\"unom\"].apply(int)) & set(table_3[\"UNOM\"].apply(int)))\n",
    "        self.dict_all_works = {name: count for count,name in enumerate(set(table_3[\"WORK_NAME\"]))}\n",
    "        self.dict_all_works_zero = {name: 0 for name in set(table_3[\"WORK_NAME\"])}\n",
    "        self.table_1 = table_1\n",
    "        self.table_2 = table_2\n",
    "        self.table_3 = table_3\n",
    "        self.padded = padded #pd.Dself.encoder_text(table_2[\"Наименование\"])\n",
    "\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.unique_unom)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        unom = self.unique_unom[idx]\n",
    "        # idx = unom\n",
    "        # get needed table\n",
    "\n",
    "        table_1_unom = table_1.loc[(table_1[\"COL_782\"].apply(int)==unom)]\n",
    "        table_2_unom = table_2.loc[(table_2[\"unom\"].apply(int)==unom)]\n",
    "        table_3_unom = table_3.loc[(table_3[\"UNOM\"].apply(int)==unom)]\n",
    "\n",
    "        work_unom_time = self.get_difference_time_work(table_3_unom)\n",
    "\n",
    "        table_3_unom['WORK_NAME'] = table_3_unom['WORK_NAME'].apply((lambda x: self.dict_all_works[x]))\n",
    "        # choose needed coloumn in table\n",
    "        table_1_res = self.get_table(table_1_unom, self.features_table_1).apply(float).to_numpy()\n",
    "        table_2_res = self.get_table(table_2_unom, self.features_table_2)\n",
    "        table_2_res = self.convert_date(table_2_res)\n",
    "        #print(\"table_2_res\", table_2_res[0][0])\n",
    "        table_3_res = self.get_table(table_3_unom, self.features_table_3)\n",
    "        #print(\"table_3_res\", table_3_res)\n",
    "        table_2_text = self.get_padded_sent(unom)\n",
    "        #print(\"table_2_text\", table_2_text)\n",
    "        # get right result\n",
    "        print(work_unom_time)\n",
    "        #work_unom_time = self.get_difference_time_work(table_3_unom)\n",
    "        #print(table_1_res, table_2_res, table_3_res)\n",
    "        X_batch.append((table_1_res, table_2_res, table_3_res, table_2_text))\n",
    "        y_batch.append(pd.DataFrame(work_unom_time).to_numpy())\n",
    "        return  0,0#torch.tensor((table_1_res, table_2_res, table_3_res, table_2_text)), torch.tensor(work_unom_time)\n",
    "    \n",
    "\n",
    "    def convert_date(self, table):\n",
    "        new_t = {\"day_beg\": [], \"month_beg\": [], \"year_beg\": [], \"day_end\": [], \"month_end\": [], \"year_end\": []}\n",
    "        new_t[\"day_beg\"] = table[\"Дата создания во внешней системе\"].dt.day.to_numpy()\n",
    "        new_t[\"month_beg\"] = table[\"Дата создания во внешней системе\"].dt.month.to_numpy()\n",
    "        new_t[\"year_beg\"]  = table[\"Дата создания во внешней системе\"].dt.year.to_numpy()\n",
    "        new_t[\"hour_beg\"]  = table[\"Дата создания во внешней системе\"].dt.hour.to_numpy()\n",
    "        new_t[\"minute_beg\"]  = table[\"Дата создания во внешней системе\"].dt.minute.to_numpy()\n",
    "        new_t[\"day_end\"] = table[\"Дата закрытия\"].dt.day.to_numpy()\n",
    "        new_t[\"month_end\"] = table[\"Дата закрытия\"].dt.month.to_numpy()\n",
    "        new_t[\"year_end\"] = table[\"Дата закрытия\"].dt.year.to_numpy()\n",
    "        new_t[\"hour_end\"] = table[\"Дата закрытия\"].dt.hour.to_numpy()\n",
    "        new_t[\"minute_end\"] = table[\"Дата закрытия\"].dt.minute.to_numpy()\n",
    "        return pd.DataFrame(new_t)\n",
    "    \n",
    "\n",
    "    def get_padded_sent(self, unom: int):\n",
    "        index = self.table_2[self.table_2['unom'].apply(int) == unom].index \n",
    "        pad_sent = []\n",
    "        for idx in index:\n",
    "            pad_sent.append(self.padded[idx])\n",
    "        return np.asarray(pad_sent)\n",
    "\n",
    "\n",
    "    def get_table(self, table, features):\n",
    "        dict_new_table = {name: [] for name in features}\n",
    "        for name in features:\n",
    "            dict_new_table[name] = table[name]\n",
    "        return pd.DataFrame(dict_new_table)\n",
    "    \n",
    "\n",
    "    def get_difference_time_work(self, table): # передаем данные по конкретному уному\n",
    "        # get list all work\n",
    "        work_unom_time = self.dict_all_works_zero\n",
    "        # get date fact and plan work\n",
    "        fact_beg_day = table[\"FACT_DATE_START\"].dt.day\n",
    "        fact_beg_month = table[\"FACT_DATE_START\"].dt.month\n",
    "        fact_beg_year  = table[\"FACT_DATE_START\"].dt.year\n",
    "        plan_beg_day = table[\"PLAN_DATE_START\"].dt.day\n",
    "        plan_beg_month = table[\"PLAN_DATE_START\"].dt.month\n",
    "        plan_beg_year = table[\"PLAN_DATE_START\"].dt.year\n",
    "        # compute result difference \n",
    "        result = ((plan_beg_year - fact_beg_year) * 365 + (plan_beg_month - fact_beg_month) * 30 + (plan_beg_day - fact_beg_day)).to_numpy()\n",
    "        # compute mean time deviation for all works\n",
    "        for count, name in enumerate(table[\"WORK_NAME\"]):\n",
    "            work_unom_time[name] =  (work_unom_time[name] + result[count]) / 2\n",
    "        values = []\n",
    "        for key in work_unom_time.keys():\n",
    "            values.append(work_unom_time[key])\n",
    "        return values\n",
    "    \n",
    "\n",
    "    def encoder_text(self, coloumn_text):\n",
    "        # apply tokenizer\n",
    "        with torch.no_grad():\n",
    "            coloumn = coloumn_text.apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))\n",
    "        # <pad>\n",
    "        max_len = 0\n",
    "        for i in coloumn.values:\n",
    "            if len(i) > max_len:\n",
    "                max_len = len(i)\n",
    "\n",
    "        if max_len % 2 == 1:\n",
    "            max_len += 1\n",
    "\n",
    "        padded = np.array([i + [0]*(max_len-len(i)) for i in coloumn.values])\n",
    "\n",
    "        return pd.DataFrame(padded)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, dim_model, dropout, max_len):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        # Info\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Encoding - From formula\n",
    "        pos_encoding = torch.zeros(max_len, dim_model)\n",
    "        positions_list = torch.arange(0, max_len, dtype=torch.float).view(-1, 1)\n",
    "        division_term = torch.exp(torch.arange(0, dim_model, 2).float() * (-np.log(10000.0)) / dim_model) \n",
    "\n",
    "        # PE(pos, 2i) = = sin(pos/1000^(2i/dim_model))\n",
    "        pos_encoding[:, 0::2] = torch.sin(positions_list * division_term)\n",
    "\n",
    "        # PE(pos, 2i + 1) = cos(pos/1000^(2i/dim_model))\n",
    "        pos_encoding[:, 1::2] = torch.cos(positions_list * division_term)\n",
    "\n",
    "        pos_encoding = pos_encoding.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer(\"pos_encoding\", pos_encoding)\n",
    "    \n",
    "\n",
    "    def forward(self, token_embedding):\n",
    "        output = self.dropout(token_embedding + self.pos_encoding[:token_embedding.size(0), :])\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Каждая заявка вносит свой вклад в приближение капитального ремонта\n",
    "class ProcessRequestsNN(nn.Module):\n",
    "    def __init__(self, num_features: int, num_add_features: int, hidden_dim: int = 64, emb_dim: int = 1024, modelBERT = None, num_requests: int = 19,\n",
    "                 dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.num_features = num_features\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding_size = emb_dim\n",
    "        self.num_requests = num_requests\n",
    "        self.output_conv = self.__compute_output() + num_add_features\n",
    "        # Layers\n",
    "        # Embedding\n",
    "        if modelBERT is None:\n",
    "            # make Bert Model for make embedding requests\n",
    "            model_class, tokenizer_class, pretrained_weights = (ppb.DistilBertModel, ppb.DistilBertTokenizer, 'distilbert-base-uncased')\n",
    "            self.embed = model_class.from_pretrained(pretrained_weights)\n",
    "            self.dimBERT = 768\n",
    "        else:\n",
    "            self.embed = modelBERT\n",
    "            self.dimBERT = 768\n",
    "        \n",
    "        self.conv_1 = nn.Conv1d(in_channels=1, out_channels=64, kernel_size=5, stride=2, padding=4)\n",
    "        self.max_pool_1 = nn.MaxPool1d(kernel_size=3, stride=1, padding=1)\n",
    "        self.conv_2 = nn.Conv1d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.max_pool_2 = nn.MaxPool1d(kernel_size=3, stride=1, padding=1)\n",
    "        self.conv_3 = nn.Conv1d(in_channels=64, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.max_pool_3 = nn.MaxPool1d(kernel_size=3, stride=1, padding=1)\n",
    "        self.conv_4 = nn.Conv1d(in_channels=32, out_channels=1, kernel_size=3, stride=1, padding=1)\n",
    "        self.max_pool_4 = nn.MaxPool1d(kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "\n",
    "        self.embed_after_conv = nn.Embedding(self.output_conv , self.embedding_size)\n",
    "        self.linear_1 = nn.Linear(self.output_conv, int(self.embedding_size/2))\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear_2 = nn.Linear(int(self.embedding_size/2), int(self.embedding_size/8))\n",
    "        self.linear_3 = nn.Linear(int(self.embedding_size/8), self.num_requests)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.batch_norm = nn.BatchNorm1d(self.output_conv)\n",
    "\n",
    "    \n",
    "    def forward(self, in_data, in_data_2):\n",
    "        attention_mask = torch.tensor(np.where(in_data != 0, 1, 0))\n",
    "        #embed = model_class.from_pretrained(pretrained_weights)\n",
    "        with torch.no_grad():\n",
    "            out = self.embed(in_data, attention_mask=attention_mask)\n",
    "\n",
    "        out = out[0].reshape(out[0].shape[0], out[0].shape[1]*out[0].shape[2]).unsqueeze(1)\n",
    "        print(\"out.shape\", out.shape)\n",
    "\n",
    "        # Apply conv layer\n",
    "        out = self.conv_1(out)\n",
    "        out = self.max_pool_1(out)\n",
    "        out = self.conv_2(out)\n",
    "        out = self.max_pool_2(out)\n",
    "        out = self.conv_3(out)\n",
    "        out = self.max_pool_3(out)\n",
    "        out = self.conv_4(out)\n",
    "        out = self.max_pool_4(out)\n",
    "        out = self.flatten(out)\n",
    "\n",
    "        # concatenate date and name requests\n",
    "        out = torch.concatenate(((out, in_data_2.permute(1,0))), dim=1)\n",
    "        out = out.unsqueeze(0)\n",
    "        # Apply Fully connected layer\n",
    "        #out = self.batch_norm(out)\n",
    "        #out = self.embed_after_conv(out.Lomnn)\n",
    "        out = self.linear_1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.linear_2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.linear_3(out)\n",
    "\n",
    "        return out\n",
    "    \n",
    "\n",
    "    def __compute_output(self):\n",
    "        dim1 = 26882 \n",
    "        dim_out = dim1\n",
    "        return dim_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "buff_out = 0\n",
    "buff_h = 0\n",
    "buff_c = 0\n",
    "buff_cat = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComputeResultNN(nn.Module):\n",
    "    def __init__(self, num_features_req: int, num_features_cat: int = 14, num_output: int = 19,\n",
    "                 emb_size: int = 128, hidden_dim: int = 64, dropout: float = 0.1, num_add_features: int = 10,\n",
    "                 num_features_date_cup:int = 4):\n",
    "        super().__init__()\n",
    "        self.num_features_cat = num_features_cat\n",
    "        self.emb_size = emb_size\n",
    "        self.hid_dim = hidden_dim\n",
    "\n",
    "        self.process_request = ProcessRequestsNN(num_features=num_features_req,\n",
    "                                                 num_add_features=num_add_features)\n",
    "        \n",
    "        self.rnn = nn.LSTM(input_size = num_output,\n",
    "                           hidden_size = self.hid_dim,\n",
    "                           num_layers = 1, # may be 4 or 5 поставить\n",
    "                           dropout=dropout,\n",
    "                           bidirectional=True) #через lstm прогоняем все запросы получаем скрытое представление\n",
    "        self.linear_1 = nn.Linear(num_features_cat + num_output, self.hid_dim)\n",
    "        self.linear_2 = nn.Linear(self.hid_dim, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "\n",
    "    def forward(self, in_data_cat, in_data_request, in_data_date):\n",
    "        # proccess request\n",
    "        out_req = self.process_request(in_data_request, in_data_date).squeeze(0)\n",
    "        #out_req, (hid, cell) = self.rnn(out_req)\n",
    "        # concatenate information\n",
    "        multiply = torch.tensor([1 for i in range(19)])\n",
    "        for i in out_req:\n",
    "            multiply = torch.multiply(multiply, i)\n",
    "        print(multiply.size())\n",
    "        data_concat = torch.concatenate((multiply, in_data_cat), dim=0)\n",
    "\n",
    "        # proccess specifications and out other model\n",
    "        out = self.linear_1(data_concat.float())\n",
    "        out = self.relu(out)\n",
    "        out = self.linear_2(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 70\n",
    "NUM_FEATURES_REQ = MAX_LEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "c:\\Anaconda\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "# make model\n",
    "model = ComputeResultNN(num_features_req=NUM_FEATURES_REQ,\n",
    "                        num_features_cat=15,\n",
    "                        num_output=19,\n",
    "                        num_add_features=10,\n",
    "                        num_features_date_cup=7)\n",
    "loss_fn = nn.L1Loss()\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# получение даты и ее разности для запросов\n",
    "def get_data_date(table):\n",
    "    days_close = table[\"day_end\"]\n",
    "\n",
    "    time_perform_days = table[\"day_end\"].to_numpy() - table[\"day_beg\"]\n",
    "    time_perform_month = table[\"month_end\"].to_numpy() - table[\"month_beg\"].to_numpy()\n",
    "    time_perform_year = table[\"year_end\"].to_numpy() - table[\"year_beg\"].to_numpy()\n",
    "    time_perform_hour = table[\"hour_end\"].to_numpy() - table[\"hour_beg\"].to_numpy()\n",
    "    time_perform_minute = table[\"minute_end\"].to_numpy() - table[\"minute_beg\"].to_numpy()\n",
    "\n",
    "    arr_times = [table[\"day_beg\"].to_numpy(), table[\"month_beg\"].to_numpy(), table[\"year_beg\"].to_numpy(), \n",
    "                 table[\"hour_beg\"].to_numpy(), table[\"minute_beg\"].to_numpy(),time_perform_days,\n",
    "                 time_perform_month, time_perform_year,\n",
    "                 time_perform_hour,time_perform_minute]\n",
    "    return torch.tensor(arr_times)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# вычисление ответа\n",
    "def compute_y(table):\n",
    "    d = {f'{i}': 0 for i in range(19)}\n",
    "    d_exists = {f'{i}': 0 for i in range(19)}\n",
    "    d_out = {}\n",
    "    fact_beg_day = table[\"FACT_DATE_START\"].dt.day.to_numpy()\n",
    "    fact_beg_month = table[\"FACT_DATE_START\"].dt.month.to_numpy()\n",
    "    fact_beg_year  = table[\"FACT_DATE_START\"].dt.year.to_numpy()\n",
    "    plan_beg_day = table[\"PLAN_DATE_START\"].dt.day.to_numpy()\n",
    "    plan_beg_month = table[\"PLAN_DATE_START\"].dt.month.to_numpy()\n",
    "    plan_beg_year = table[\"PLAN_DATE_START\"].dt.year.to_numpy()\n",
    "    result = ((plan_beg_year - fact_beg_year) * 365 + (plan_beg_month - fact_beg_month) * 30 + (plan_beg_day - fact_beg_day))\n",
    "    \n",
    "    table = table.to_numpy()\n",
    "    for count, name in enumerate(table):\n",
    "        d[f\"{name[0]}\"] = (d[f\"{name[0]}\"] + result[count]) / 2\n",
    "        d_exists[f\"{name[0]}\"] = 1\n",
    "    for k in d.keys():\n",
    "        if d_exists[k] == 1:\n",
    "            d_out[k] = d[k]\n",
    "    return d_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\843E~1\\AppData\\Local\\Temp/ipykernel_6188/6310117.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  table_3_unom['WORK_NAME'] = table_3_unom['WORK_NAME'].apply((lambda x: self.dict_all_works[x]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, -12.5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "{'7': -12.5}\n",
      "torch.Size([193, 70]) torch.Size([10, 193])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\843E~1\\AppData\\Local\\Temp/ipykernel_6188/4294681698.py:15: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:248.)\n",
      "  return torch.tensor(arr_times)\n",
      "C:\\Users\\843E~1\\AppData\\Local\\Temp/ipykernel_6188/2775317569.py:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  in_data_date = torch.tensor(data_add_for_first_model)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out.shape torch.Size([193, 1, 53760])\n",
      "torch.Size([19])\n",
      "3\n",
      "data_concat.size() torch.Size([34])\n",
      "4\n",
      "[-2.0, -20.5, 0, 0, -125.5, 0, 0, -12.5, -2.0, -17.0, 0, 0, -20.5, 0.5, -2.0, 43.0, 0, -20.5, 0]\n",
      "{'0': -2.0, '1': -20.5, '4': -125.5, '8': -2.0, '9': -17.0, '12': -20.5, '13': 0.5, '14': -2.0, '15': 43.0, '17': -20.5}\n",
      "torch.Size([141, 70]) torch.Size([10, 141])\n",
      "out.shape torch.Size([141, 1, 53760])\n",
      "torch.Size([19])\n",
      "3\n",
      "data_concat.size() torch.Size([34])\n",
      "4\n",
      "torch.Size([141, 70]) torch.Size([10, 141])\n",
      "out.shape torch.Size([141, 1, 53760])\n",
      "torch.Size([19])\n",
      "3\n",
      "data_concat.size() torch.Size([34])\n",
      "4\n",
      "torch.Size([141, 70]) torch.Size([10, 141])\n",
      "out.shape torch.Size([141, 1, 53760])\n",
      "torch.Size([19])\n",
      "3\n",
      "data_concat.size() torch.Size([34])\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "dataset = CustomDataset(table_1, table_2, table_3, padded)\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=1)\n",
    "X_batch = []\n",
    "y_batch = []\n",
    "N_EPOCHS = 5\n",
    "BATCH_SIZE = 128\n",
    "c = 0\n",
    "train_epoch_loss = []\n",
    "val_epoch_loss = []\n",
    "\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    for x,y in dataloader:\n",
    "        for count, x_batch in enumerate(X_batch):\n",
    "            y = compute_y(x_batch[2])\n",
    "            print(y)\n",
    "            for work_id in y.keys():\n",
    "                data_request = x_batch[3] #данные по заявкам\n",
    "                data_add_for_first_model = get_data_date(x_batch[1]) # данные времени по заявкам\n",
    "                data_mlh = x_batch[0] # хар-ки дома\n",
    "                number_work = float(work_id) # номер работы\n",
    "                y_true = y[work_id]\n",
    "\n",
    "                #in_data_cat - категориальные признаки + дата окончания последнего кап ремонта\n",
    "                in_data_cat = torch.cat((torch.tensor(data_mlh), torch.tensor([int(work_id)])))\n",
    "\n",
    "                # in_data_requests - заявки по данному уному\n",
    "                in_data_requests = torch.tensor(data_request)\n",
    "                # in_data_date - данные о времени заявок\n",
    "                in_data_date = torch.tensor(data_add_for_first_model)\n",
    "                print(in_data_requests.size(), in_data_date.size())\n",
    "\n",
    "                preds = model(in_data_cat, in_data_requests, in_data_date)\n",
    "\n",
    "                opt.zero_grad()\n",
    "                loss = loss_fn(torch.tensor([y_true]), preds)\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "                train_loss.append(loss)\n",
    "        X_batch = []\n",
    "        y_batch = []\n",
    "    train_loss.append(sum(train_loss)/len(train_loss))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save/Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"predict_dev.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(checkpoint_path, model, optimizer):\n",
    "    # state_dict: a Python dictionary object that:\n",
    "    # - for a model, maps each layer to its parameter tensor;\n",
    "    # - for an optimizer, contains info about the optimizer’s states and hyperparameters used.\n",
    "    state = {\n",
    "        'state_dict': model.state_dict(),\n",
    "        'optimizer' : optimizer.state_dict()}\n",
    "    torch.save(state, checkpoint_path)\n",
    "    print('model saved to %s' % checkpoint_path)\n",
    "    \n",
    "def load_checkpoint(checkpoint_path, model, optimizer):\n",
    "    state = torch.load(checkpoint_path)\n",
    "    model.load_state_dict(state['state_dict'])\n",
    "    optimizer.load_state_dict(state['optimizer'])\n",
    "    print('model loaded from %s' % checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new model\n",
    "save_checkpoint(\"predict_dev.pth\", model, opt)\n",
    "model = model = ComputeResultNN(num_features_req=NUM_FEATURES_REQ,\n",
    "                        num_features_cat=14,\n",
    "                        num_output=19,\n",
    "                        num_add_features=10,\n",
    "                        num_features_date_cup=7)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=0.001, momentum=0.9)\n",
    "# load from the final checkpoint\n",
    "load_checkpoint('<CHECKPOINT NAME>', model, opt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_padded(table_2, maxl_len=70):\n",
    "    #BERT tokenizer\n",
    "    model_class, tokenizer_class, pretrained_weights = (ppb.DistilBertModel, ppb.DistilBertTokenizer, 'distilbert-base-uncased')\n",
    "    tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
    "\n",
    "    # удаление нахер вссего NaN дерьма\n",
    "    for count, elem in enumerate(table_2[\"Наименование\"].isna()):\n",
    "        if elem:\n",
    "            table_2 = table_2.drop(labels=[count], axis=0)\n",
    "    table_2[\"Наименование\"].isna().sum()\n",
    "\n",
    "    # apply tokenizator\n",
    "    df = table_2[\"Наименование\"].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))\n",
    "\n",
    "    # find max_len\n",
    "    max_len = 0\n",
    "    for i in df.values:\n",
    "        if len(i) > max_len:\n",
    "            max_len = len(i)\n",
    "\n",
    "    if max_len % 2 == 1:\n",
    "        max_len += 1\n",
    "\n",
    "    # add <pad> symbol\n",
    "    padded = np.array([i + [0]*(max_len-len(i)) for i in df.values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#in_data_cat - категориальные признаки + дата окончания последнего кап ремонта (14 штук)\n",
    "# in_data_requests - заявки по данному уному (заявки по уному в виде таблицы)\n",
    "# in_data_date - данные о времени заявок (3 т)\n",
    "def get_result(table_1, table_2, table_3, model):\n",
    "    model.train(False)\n",
    "    result = {}\n",
    "    X_batch = []\n",
    "    y_batch = []\n",
    "    padded = get_padded(table_2)\n",
    "    dataset = CustomDataset(table_1, table_2, table_3, padded)\n",
    "    dataloader = DataLoader(dataset=dataset, batch_size=1)\n",
    "    for x,_ in dataloader:\n",
    "        for count, x_batch in enumerate(X_batch):\n",
    "            y = compute_y(x_batch[2])\n",
    "            print(y)\n",
    "            for work_id in y.keys():\n",
    "                data_request = x_batch[3] #данные по заявкам\n",
    "                data_add_for_first_model = get_data_date(x_batch[1]) # данные времени по заявкам\n",
    "                data_mlh = x_batch[0] # хар-ки дома\n",
    "                number_work = float(work_id) # номер работы\n",
    "                y_true = y[work_id]\n",
    "\n",
    "                #in_data_cat - категориальные признаки + дата окончания последнего кап ремонта\n",
    "                in_data_cat = torch.cat((torch.tensor(data_mlh), torch.tensor([int(work_id)])))\n",
    "\n",
    "                # in_data_requests - заявки по данному уному\n",
    "                in_data_requests = torch.tensor(data_request)\n",
    "                # in_data_date - данные о времени заявок\n",
    "                in_data_date = torch.tensor(data_add_for_first_model)\n",
    "\n",
    "                preds = model(in_data_cat, in_data_requests, in_data_date)\n",
    "\n",
    "                result[work_id] = preds\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def additional_training(table_1, table_2, table_3, opt, model):\n",
    "    model.train(True)\n",
    "    loss_fn = nn.L1Loss()\n",
    "    padded = get_padded(table_2)\n",
    "    dataset = CustomDataset(table_1, table_2, table_3, padded)\n",
    "    dataloader = DataLoader(dataset=dataset, batch_size=1)\n",
    "    for x,_ in dataloader:\n",
    "        for count, x_batch in enumerate(X_batch):\n",
    "            y = compute_y(x_batch[2])\n",
    "            print(y)\n",
    "            for work_id in y.keys():\n",
    "                data_request = x_batch[3] #данные по заявкам\n",
    "                data_add_for_first_model = get_data_date(x_batch[1]) # данные времени по заявкам\n",
    "                data_mlh = x_batch[0] # хар-ки дома\n",
    "                number_work = float(work_id) # номер работы\n",
    "                y_true = y[work_id]\n",
    "\n",
    "                #in_data_cat - категориальные признаки + дата окончания последнего кап ремонта\n",
    "                in_data_cat = torch.cat((torch.tensor(data_mlh), torch.tensor([int(work_id)])))\n",
    "\n",
    "                # in_data_requests - заявки по данному уному\n",
    "                in_data_requests = torch.tensor(data_request)\n",
    "                # in_data_date - данные о времени заявок\n",
    "                in_data_date = torch.tensor(data_add_for_first_model)\n",
    "\n",
    "                preds = model(in_data_cat, in_data_requests, in_data_date)\n",
    "\n",
    "                opt.zero_grad()\n",
    "                loss = loss_fn(torch.tensor(y_true), preds)\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "    model.train(False)\n",
    "    return model, opt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
